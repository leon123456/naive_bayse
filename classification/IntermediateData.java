package com.kuaishou.old.antispam.classification;

import java.io.BufferedReader;
import java.io.File;
import java.io.FileInputStream;
import java.io.FileNotFoundException;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.InputStreamReader;
import java.io.ObjectOutputStream;
import java.io.Serializable;
import java.nio.charset.Charset;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.HashSet;
import java.util.List;
import java.util.Map.Entry;

import org.apache.commons.io.FileUtils;

import com.kuaishou.old.split.ChineseSpliter;
import com.kuaishou.old.split.ChineseSpliterSmart;
import com.kuaishou.old.util.StringEscapeUtil;

// CHECKSTYLE:OFF
public class IntermediateData implements Serializable {

    /**
     * auto generated by eclipse.
     */
    private static final long serialVersionUID = -420389048890617397L;

    /** 类别名. */
    public String[] classifications;

    /** 单词X在类别C下出现的总数. */
    public HashMap[] filesOfXC;
    /** 给定分类下的文件数目. */
    public int[] filesOfC;
    /** 根目录下的文件总数. */
    public int files;

    /** 单词X在类别C下出现的总数 */
    public HashMap[] tokensOfXC;
    /** 类别C下所有单词的总数. */
    public int[] tokensOfC;
    /** 整个语料库中单词的总数. */
    public int tokens;
    /** 整个训练语料所出现的单词. */
    public HashSet<String> vocabulary;

    /** 文本分类语料的根目录. */
    private transient String dir;
    /** 语料库中的文本文件的字符编码. */
    private transient String encoding;

    public IntermediateData() {
        vocabulary = new HashSet<>();
    }

    /**
     * 预计算，产生中间结果，存放到磁盘中.
     *
     * @param trainTextDir
     *            已经分类的语料库，结构为
     * trainnedTextDir
     *            ├- 类别1\
     *                    ├- 文件1.txt
     *                    ├- 文件2.txt ...
     *            ├- 类别2\
     *                    ├- 文件1.txt
     *                    ├- 文件2.txt ...
     * @param txtEncoding
     *            语料库中的文本文件编码
     * @throws FileNotFoundException
     */
    public final void generate(final String trainTextDir, final String txtEncoding,
            final String modelFile) throws FileNotFoundException {
        // 一些初始化动作
        dir = trainTextDir;
        if (txtEncoding == null) {
            encoding = "GBK"; // 默认文本文件的编码为GBK;
        } else {
            encoding = txtEncoding;
        }

        // 枚举目录，获得各个类名
        File tmpDir = new File(dir);
        System.out.println(dir);
        System.out.println(tmpDir.exists());
        if (!tmpDir.isDirectory()) {
            throw new IllegalArgumentException("训练语料库搜索失败！ [" + dir + "]");
        }
        classifications = tmpDir.list();

        filesOfC = new int[classifications.length];
        filesOfXC = new HashMap[classifications.length];
        tokensOfC = new int[classifications.length];
        tokensOfXC = new HashMap[classifications.length];
        for (int i = 0; i < classifications.length; i++) {
            tokensOfXC[i] = new HashMap<String, Integer>();
            filesOfXC[i] = new HashMap<String, Integer>();
        }

        // 计算各个类别的文件总数，保存
        for (int i = 0; i < classifications.length; i++) {
            int n = calcFileCountOfClassification(i);
            filesOfC[i] = n;
            files += n; // 计算文件总数，保存
        }

        // 获得单词表
        try {
            extractVocabulary();
        } catch (IOException e1) {
            e1.printStackTrace();
        }

        // 计算各类别下单词总数，单词总数
        try {
            calculate();
        } catch (IOException e1) {
            e1.printStackTrace();
        }

        // 将预处理后的数据写入到磁盘
        try {
            ObjectOutputStream out = new ObjectOutputStream(new FileOutputStream(modelFile));
            out.writeObject(this);
            out.close();
        } catch (IOException e) {
            e.printStackTrace();
        }

        System.out.println();
    }

    public final void generate(final String trainTextDir, final String txtEncoding,
            final String modelFile, String flag) throws FileNotFoundException {
        // 一些初始化动作
        dir = trainTextDir;
        if (txtEncoding == null) {
            encoding = "GBK"; // 默认文本文件的编码为GBK;
        } else {
            encoding = txtEncoding;
        }

        // 枚举目录，获得各个类名
        File tmpDir = new File(dir);
        System.out.println(dir);
        System.out.println(tmpDir.exists());
        if (!tmpDir.isDirectory()) {
            throw new IllegalArgumentException("训练语料库搜索失败！ [" + dir + "]");
        }

        classifications = tmpDir.list();

        filesOfC = new int[classifications.length];
        filesOfXC = new HashMap[classifications.length];
        tokensOfC = new int[classifications.length];
        tokensOfXC = new HashMap[classifications.length];
        for (int i = 0; i < classifications.length; i++) {
            tokensOfXC[i] = new HashMap<String, Integer>();
            filesOfXC[i] = new HashMap<String, Integer>();
        }

        // 计算各个类别的文件总数，保存
        for (int i = 0; i < classifications.length; i++) {
            int n = calcFileCountOfClassification(i);
            filesOfC[i] = n;
            files += n; // 计算文件总数，保存
        }

        // 获得单词表
        try {
            extractVocabulary(flag);
        } catch (IOException e1) {
            e1.printStackTrace();
        }

        // 计算各类别下单词总数，单词总数
        try {
            calculate(flag);
        } catch (IOException e1) {
            e1.printStackTrace();
        }

        // 将预处理后的数据写入到磁盘
        try {
            ObjectOutputStream out = new ObjectOutputStream(new FileOutputStream(modelFile));
            out.writeObject(this);
            out.close();
        } catch (IOException e) {
            e.printStackTrace();
        }

        System.out.println();
    }

    public final void generateTwoTerms(final String trainTextDir, final String txtEncoding,
            final String modelFile) throws FileNotFoundException {
        // 一些初始化动作
        dir = trainTextDir;
        if (txtEncoding == null) {
            encoding = "GBK"; //// 默认文本文件的编码为GBK;
        } else {
            encoding = txtEncoding;
        }

        // 枚举目录，获得各个类名
        File tmpDir = new File(dir);
        System.out.println(dir);
        System.out.println(tmpDir.exists());
        if (!tmpDir.isDirectory()) {
            throw new IllegalArgumentException("训练语料库搜索失败！ [" + dir + "]");
        }
        classifications = tmpDir.list();

        filesOfC = new int[classifications.length];
        filesOfXC = new HashMap[classifications.length];
        tokensOfC = new int[classifications.length];
        tokensOfXC = new HashMap[classifications.length];
        for (int i = 0; i < classifications.length; i++) {
            tokensOfXC[i] = new HashMap<String, Integer>();
            filesOfXC[i] = new HashMap<String, Integer>();
        }

        // 计算各个类别的文件总数，保存
        for (int i = 0; i < classifications.length; i++) {
            int n = calcFileCountOfClassification(i);
            filesOfC[i] = n;
            files += n; // 计算文件总数，保存
        }

        // 获得单词表
        try {
            extractVocabularyTwoTerms();
        } catch (IOException e1) {
            e1.printStackTrace();
        }

        // 计算各类别下单词总数，单词总数
        try {
            calculateTwoTerms();
        } catch (IOException e1) {
            e1.printStackTrace();
        }

        // 将预处理后的数据写入到磁盘
        try {
            ObjectOutputStream out = new ObjectOutputStream(new FileOutputStream(modelFile));
            out.writeObject(this);
            out.close();
        } catch (IOException e) {
            e.printStackTrace();
        }

        System.out.println();
    }

    public final void generateTwoTerms(final String trainTextDir, final String txtEncoding,
            final String modelFile, String flag) throws FileNotFoundException {
        // 一些初始化动作
        dir = trainTextDir;
        if (txtEncoding == null) {
            encoding = "GBK"; //// 默认文本文件的编码为GBK;
        } else {
            encoding = txtEncoding;
        }

        // 枚举目录，获得各个类名
        File tmpDir = new File(dir);
        System.out.println(dir);
        System.out.println(tmpDir.exists());
        if (!tmpDir.isDirectory()) {
            throw new IllegalArgumentException("训练语料库搜索失败！ [" + dir + "]");
        }
        classifications = tmpDir.list();

        filesOfC = new int[classifications.length];
        filesOfXC = new HashMap[classifications.length];
        tokensOfC = new int[classifications.length];
        tokensOfXC = new HashMap[classifications.length];
        for (int i = 0; i < classifications.length; i++) {
            tokensOfXC[i] = new HashMap<String, Integer>();
            filesOfXC[i] = new HashMap<String, Integer>();
        }

        // 计算各个类别的文件总数，保存
        for (int i = 0; i < classifications.length; i++) {
            int n = calcFileCountOfClassification(i);
            filesOfC[i] = n;
            files += n; // 计算文件总数，保存
        }

        // 获得单词表
        try {
            extractVocabularyTwoTerms(flag);
        } catch (IOException e1) {
            e1.printStackTrace();
        }

        // 计算各类别下单词总数，单词总数
        try {
            calculateTwoTerms(flag);
        } catch (IOException e1) {
            e1.printStackTrace();
        }

        // 将预处理后的数据写入到磁盘
        try {
            ObjectOutputStream out = new ObjectOutputStream(new FileOutputStream(modelFile));
            out.writeObject(this);
            out.close();
        } catch (IOException e) {
            e.printStackTrace();
        }

        System.out.println();
    }

    /** 获得单词表.
     * @throws IOException 读取语料库出错
     */
    private void extractVocabulary() throws IOException {
        for (String c : classifications) {
            String[] filesPath = getFilesPath(dir, c);

            for (String file : filesPath) {
                //String text = getText(file, encoding);
                List<String> texts = getTextList(file, encoding);
                for (String text : texts) {
                    String[] terms;
                    // 中文分词处理(分词后结果可能还包含有停用词）
                    text = StringEscapeUtil.filterByRegEx(text.trim());
                    terms = ChineseSpliter.split(text, " ").split(" ");
                    terms = ChineseSpliter.dropStopWords(terms); // 去掉停用词，以免影响分类

                    for (String term : terms) { //去除重复单词
                        vocabulary.add(term);
                    }
                }
            }
        }
    }

    private void extractVocabulary(String flag) throws IOException {
        for (String c : classifications) {
            String[] filesPath = getFilesPath(dir, c);
            for (String file : filesPath) {
                //                List<String> texts = getTextList(file, encoding);
                File textFile = new File(file);
                List<String> texts = FileUtils.readLines(textFile,
                        Charset.forName("UTF-8"));
                int cnt = 0;
                for (String text : texts) {
                    if (text != null && text.length() > 0) {
                        String[] terms;
                        cnt++;
                        // 中文分词处理(分词后结果可能还包含有停用词）
                        terms = StringEscapeUtil.getTermsbyFilter(text, flag);
                        if (terms != null && terms.length > 0) {
                            for (String term : terms) { //去除重复单词
                                vocabulary.add(term);
                            }
                        }
                    }
                }
            }
        }
    }

    /** 获得单词表.
     * @throws IOException 读取语料库出错
     */
    private void extractVocabularyTwoTerms() throws IOException {
        for (String c : classifications) {
            String[] filesPath = getFilesPath(dir, c);

            for (String file : filesPath) {
                //String text = getText(file, encoding);
                List<String> texts = getTextList(file, encoding);
                for (String text : texts) {
                    String[] terms;
                    text = StringEscapeUtil.filterByRegEx(text);
                    terms = ChineseSpliterSmart.split(text, " ").split(" ");
                    //                     terms = ChineseSpliter.dropStopWords(terms);

                    if (terms.length >= 2) {
                        for (int i = 0; i < terms.length - 1; i++) {
                            vocabulary.add(terms[i] + terms[i + 1]);
                        }
                    }
                }
            }
        }
    }

    private void extractVocabularyTwoTerms(String flag) throws IOException {
        for (String c : classifications) {
            String[] filesPath = getFilesPath(dir, c);

            for (String file : filesPath) {
                //String text = getText(file, encoding);
                List<String> texts = getTextList(file, encoding);
                for (String text : texts) {
                    String[] terms;

                    terms = StringEscapeUtil.getTermsbyFilter(text, flag);

                    if (terms.length >= 2) {
                        for (int i = 0; i < terms.length - 1; i++) {
                            vocabulary.add(terms[i] + terms[i + 1]);
                        }
                    }
                }
            }
        }
    }

    /**
     * 返回训练文本集中在给定分类下的训练文本数目.
     *
     * @param c
     *            给定的分类
     * @return 训练文本集中在给定分类下的训练文本数目
     * @throws FileNotFoundException
     */
    private int calcFileCountOfClassification(final int c) throws FileNotFoundException {
        int lines = 0;

        File classDir = new File(dir + File.separator + classifications[c]);
        for (File file : classDir.listFiles()) {
            if (!file.getName().contains("swp")) {
                System.out.println(file.getAbsolutePath());
                BufferedReader bufferedReader = new BufferedReader(
                        new InputStreamReader(new FileInputStream(file)));
                try {
                    while (bufferedReader.readLine() != null) {
                        lines++;
                    }
                } catch (IOException e) {
                    e.printStackTrace();
                }
                //lines +=  bufferedReader.lines().count();
            }
        }

        return lines;
    }

    /**
     * 计算 fileCountOfXC, fileCountOfC, fileCount, tokensOfXC, tokensOfC, tokens.
     *
     * @throws IOException
     */
    private void calculate() throws IOException {
        for (int i = 0; i < classifications.length; i++) {
            HashMap<String, Integer> tmpT = tokensOfXC[i];
            HashMap<String, Integer> tmpF = filesOfXC[i];

            String[] filesPath = getFilesPath(dir, classifications[i]);

            //filesOfC[i] = filesPath.length;
            //files += filesOfC[i];

            HashSet<String> words = new HashSet<>();
            for (String file : filesPath) {
                List<String> textList = getTextList(file, encoding);
                for (String text : textList) {
                    words.clear();

                    String[] terms;

                    // 中文分词处理(分词后结果可能还包含有停用词）
                    text = StringEscapeUtil.filterByRegEx(text.trim());
                    terms = ChineseSpliter.split(text, " ").split(" ");
                    terms = ChineseSpliterSmart.dropStopWords(terms); // 去掉停用词，以免影响分类

                    for (String term : terms) { // 计算本类别下每个单词的出现次数
                        Integer value = tmpT.get(term);
                        if (value == null) {
                            tmpT.put(term, new Integer(1));
                        } else {
                            tmpT.put(term, value + 1);
                        }
                    }

                    // 开始计算 filesOfXC[i]
                    for (String term : terms) { //去除重复单词
                        words.add(term);
                    }
                    for (String key : words) {
                        Integer value = tmpF.get(key);
                        if (value == null) {
                            tmpF.put(key, new Integer(1));
                        } else {
                            value++;
                            tmpF.put(key, value);
                        }
                    }
                }
            }

            // 该类别下所有单词的出现总数 nC
            for (Entry<String, Integer> entry : tmpT.entrySet()) {
                tokensOfC[i] += entry.getValue();
            }

            tokens += tokensOfC[i]; // 所有单词出现总数
        }
    }

    private void calculate(String flag) throws IOException {
        for (int i = 0; i < classifications.length; i++) {
            HashMap<String, Integer> tmpT = tokensOfXC[i];
            HashMap<String, Integer> tmpF = filesOfXC[i];

            String[] filesPath = getFilesPath(dir, classifications[i]);

            //filesOfC[i] = filesPath.length;
            //files += filesOfC[i];

            HashSet<String> words = new HashSet<>();
            for (String file : filesPath) {
                List<String> textList = getTextList(file, encoding);
                for (String text : textList) {
                    if (text != null && text.length() > 0) {
                        words.clear();

                        String[] terms;

                        terms = StringEscapeUtil.getTermsbyFilter(text, flag);
                        if (terms != null && terms.length > 0) {
                            for (String term : terms) { // 计算本类别下每个单词的出现次数
                                Integer value = tmpT.get(term);
                                if (value == null) {
                                    tmpT.put(term, new Integer(1));
                                } else {
                                    tmpT.put(term, value + 1);
                                }
                            }

                            // 开始计算 filesOfXC[i]
                            for (String term : terms) { //去除重复单词
                                words.add(term);
                            }
                            for (String key : words) {
                                Integer value = tmpF.get(key);
                                if (value == null) {
                                    tmpF.put(key, new Integer(1));
                                } else {
                                    value++;
                                    tmpF.put(key, value);
                                }
                            }
                        }
                    }
                }
            }

            // 该类别下所有单词的出现总数 nC
            for (Entry<String, Integer> entry : tmpT.entrySet()) {
                tokensOfC[i] += entry.getValue();
            }

            tokens += tokensOfC[i]; // 所有单词出现总数
        }
    }

    private void calculateTwoTerms() throws IOException {
        for (int i = 0; i < classifications.length; i++) {
            HashMap<String, Integer> tmpT = tokensOfXC[i];
            HashMap<String, Integer> tmpF = filesOfXC[i];

            String[] filesPath = getFilesPath(dir, classifications[i]);

            //filesOfC[i] = filesPath.length;
            //files += filesOfC[i];

            HashSet<String> words = new HashSet<>();
            for (String file : filesPath) {
                List<String> textList = getTextList(file, encoding);
                for (String text : textList) {
                    words.clear();

                    String[] terms;

                    // 中文分词处理(分词后结果可能还包含有停用词）
                    text = StringEscapeUtil.filterByRegEx(text);
                    terms = ChineseSpliterSmart.split(text, " ").split(" ");
                    //                    terms = ChineseSpliter.dropStopWords(terms); // 去掉停用词，以免影响分类

                    if (terms.length >= 2) {

                        for (int j = 0; j < terms.length - 1; j++) {//计算本类别下每个单词的出现次数
                            String key = terms[j] + terms[j + 1];
                            Integer value = tmpT.get(key);
                            if (value == null) {
                                tmpT.put(key, new Integer(1));
                            } else {
                                tmpT.put(key, value + 1);
                            }
                        }

                        // 开始计算 filesOfXC[i]
                        for (int j = 0; j < terms.length - 1; j++) {//去除重复单词
                            String key = terms[j] + terms[j + 1];
                            words.add(key);
                        }
                        for (String key : words) {
                            Integer value = tmpF.get(key);
                            if (value == null) {
                                tmpF.put(key, new Integer(1));
                            } else {
                                value++;
                                tmpF.put(key, value);
                            }
                        }
                    }

                }
            }

            // 该类别下所有单词的出现总数 nC
            for (Entry<String, Integer> entry : tmpT.entrySet()) {
                tokensOfC[i] += entry.getValue();
            }

            tokens += tokensOfC[i]; // 所有单词出现总数
        }
    }

    private void calculateTwoTerms(String flag) throws IOException {
        for (int i = 0; i < classifications.length; i++) {
            HashMap<String, Integer> tmpT = tokensOfXC[i];
            HashMap<String, Integer> tmpF = filesOfXC[i];

            String[] filesPath = getFilesPath(dir, classifications[i]);

            //filesOfC[i] = filesPath.length;
            //files += filesOfC[i];

            HashSet<String> words = new HashSet<>();
            for (String file : filesPath) {
                List<String> textList = getTextList(file, encoding);
                for (String text : textList) {
                    words.clear();

                    String[] terms;

                    // 中文分词处理(分词后结果可能还包含有停用词）
                    terms = StringEscapeUtil.getTermsbyFilter(text, flag);

                    if (terms.length >= 2) {

                        for (int j = 0; j < terms.length - 1; j++) {//计算本类别下每个单词的出现次数
                            String key = terms[j] + terms[j + 1];
                            Integer value = tmpT.get(key);
                            if (value == null) {
                                tmpT.put(key, new Integer(1));
                            } else {
                                tmpT.put(key, value + 1);
                            }
                        }

                        // 开始计算 filesOfXC[i]
                        for (int j = 0; j < terms.length - 1; j++) {//去除重复单词
                            String key = terms[j] + terms[j + 1];
                            words.add(key);
                        }
                        for (String key : words) {
                            Integer value = tmpF.get(key);
                            if (value == null) {
                                tmpF.put(key, new Integer(1));
                            } else {
                                value++;
                                tmpF.put(key, value);
                            }
                        }
                    }

                }
            }

            // 该类别下所有单词的出现总数 nC
            for (Entry<String, Integer> entry : tmpT.entrySet()) {
                tokensOfC[i] += entry.getValue();
            }

            tokens += tokensOfC[i]; // 所有单词出现总数
        }
    }

    /**
     * 根据训练文本类别返回这个类别下的所有训练文本路径(full path).
     *
     * @param dirStr 已分类的文本根目录，末尾不带斜杠
     * @param classification
     *            给定的分类
     * @return 给定分类下所有文件的路径（full path）
     */
    public static String[] getFilesPath(final String dirStr, final String classification) {
        File classDir = new File(dirStr + File.separator + classification);
        String[] ret = classDir.list();
        List<String> retList = new ArrayList<>();
        for (int i = 0; i < ret.length; i++) {
            if (!ret[i].contains("swp")) {
                ret[i] = dirStr + File.separator + classification + File.separator + ret[i];
                retList.add(ret[i]);
            }
        }
        String result[] = new String[retList.size()];
        for (int i = 0; i < result.length; i++) {
            result[i] = retList.get(i);
        }
        return result;
    }

    /**
     * 返回给定路径的文本文件内容.
     *
     * @param filePath
     *            给定的文本文件路径
     * @param encoding 文本文件的编码
     * @return 文本内容
     * @throws IOException
     *             文件找不到或IO出错
     */
    public static String getText(final String filePath, final String encoding) throws IOException {

        InputStreamReader isReader = new InputStreamReader(new FileInputStream(filePath), encoding);
        BufferedReader reader = new BufferedReader(isReader);
        String aLine;
        StringBuilder sb = new StringBuilder();

        while ((aLine = reader.readLine()) != null) {
            sb.append(aLine + " ");
        }
        isReader.close();
        reader.close();
        return sb.toString();
    }

    /**
     * 返回给定路径的文本文件内容.
     *
     * @param filePath
     *            给定的文本文件路径
     * @param encoding 文本文件的编码
     * @return 文本内容
     * @throws IOException
     *             文件找不到或IO出错
     */
    public static List<String> getTextList(final String filePath, final String encoding)
            throws IOException {

        InputStreamReader isReader = new InputStreamReader(new FileInputStream(filePath), encoding);
        BufferedReader reader = new BufferedReader(isReader);
        String aLine;
        List<String> sbList = new ArrayList<>();
        StringBuilder sb = new StringBuilder();

        while ((aLine = reader.readLine()) != null) {
            sb.append(aLine + " ");
            sbList.add(aLine);
        }
        isReader.close();
        reader.close();
        return sbList;
    }

    /** 打印命令行参数的解释信息. */
    private static void usage() {
        System.err.println("usage:\t  <语料库目录> <语料库文本编码> <中间文件>");
    }

    /**
     * 使用方法：IntermediateData d:\SogouC.mini\Sample\ gbk d:\mini.db
     * @param args
     */
    public static void main(String args[]) {
        /*if(args.length < 3) {
        	usage();
        	return;
        }
        IntermediateData tdm = new IntermediateData();
        tdm.generate(args[0], args[1], args[2]);
        */

        IntermediateData tdm = new IntermediateData();
        try {
            tdm.generate(
                    "D:/Users/Administrator/git/naivibayse/NaiveBayesClassifier/src/com/yanjiuyanjiu/text/classification/data/traindata",
                    "utf-8",
                    "D:/Users/Administrator/git/naivibayse/NaiveBayesClassifier/src/com/yanjiuyanjiu/text/classification/data/db");
        } catch (FileNotFoundException e) {
            // TODO Auto-generated catch block
            e.printStackTrace();
        }

        System.out.println("中间数据生成！");
    }
}
// CHECKSTYLE:ON
